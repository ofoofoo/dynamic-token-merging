{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torchprofile\n",
    "import time\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
    "ds = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
    "test_dataset = ds['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 100\n",
    "random_indices = random.sample(range(len(test_dataset)), subset_size)\n",
    "test_dataset = test_dataset.select(random_indices)\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')\n",
    "total_flops = 0\n",
    "total_macs = 0\n",
    "total_time = 0\n",
    "outputs = []\n",
    "targets = []\n",
    "\n",
    "for example in tqdm(test_dataset):\n",
    "    article = example['article']\n",
    "    reference_summary = example['highlights']\n",
    "    \n",
    "    inputs = tokenizer(article, \n",
    "    return_tensors='pt',\n",
    "    max_length=1024,\n",
    "    truncation=True).to(device)\n",
    "\n",
    "    vocab_size = model.config.vocab_size\n",
    "    print(f\"Vocab size: {vocab_size}\")\n",
    "    print(inputs['input_ids'].min(), inputs['input_ids'].max())\n",
    "    \n",
    "    flops_analysis = FlopCountAnalysis(model, inputs['input_ids'])\n",
    "    total_flops += flops_analysis.total()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids']\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    generated_summary = tokenizer.decode(\n",
    "        summary_ids[0], \n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    print(\"GENERATED SUMMARY\")\n",
    "    print(generated_summary)\n",
    "    print(\"TARGET SUMMARY\")\n",
    "    print(reference_summary)\n",
    "    \n",
    "    total_time += (end_time - start_time)\n",
    "    \n",
    "    outputs.append(generated_summary)\n",
    "    targets.append(reference_summary)\n",
    "\n",
    "average_flops = total_flops / len(test_dataset)\n",
    "average_time = total_time / len(test_dataset)\n",
    "\n",
    "rouge_results = rouge.compute(predictions=outputs, references=targets)\n",
    "bleu_results = bleu.compute(predictions=outputs, references=targets)\n",
    "\n",
    "print(\"Inference Results:\")\n",
    "print(f\"Average FLOPs: {average_flops:.2e}\")\n",
    "print(f\"Average forward pass time: {average_time:.4f} seconds\")\n",
    "print(\"ROUGE Scores:\", rouge_results)\n",
    "print(\"BLEU Score:\", bleu_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
