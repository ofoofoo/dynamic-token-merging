{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
    "\n",
    "ds = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
    "test_dataset = ds['test']\n",
    "\n",
    "subset_size = 100\n",
    "random_indices = random.sample(range(len(test_dataset)), subset_size)\n",
    "test_dataset = test_dataset.select(random_indices)\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')\n",
    "total_flops = 0\n",
    "total_time = 0\n",
    "outputs = []\n",
    "targets = []\n",
    "\n",
    "for example in tqdm(test_dataset):\n",
    "    article = example['article']\n",
    "    reference_summary = example['highlights']\n",
    "    tokens = tokenizer.tokenize(article, max_length=1024, truncation=True)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    token_ids_tensor = torch.tensor(token_ids).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        token_embeddings = model.model.shared(token_ids_tensor)\n",
    "    \n",
    "    importance_scores = torch.norm(token_embeddings, dim=1)\n",
    "    \n",
    "    threshold = torch.quantile(importance_scores, 0.25)\n",
    "    mask = importance_scores > threshold\n",
    "    pruned_token_ids = token_ids_tensor[mask]\n",
    "    \n",
    "    if pruned_token_ids.numel() == 0:\n",
    "        pruned_token_ids = token_ids_tensor  # Fall back to original tokens\n",
    "    \n",
    "    inputs = {\n",
    "        'input_ids': pruned_token_ids.unsqueeze(0).to(device),\n",
    "        'attention_mask': torch.ones_like(pruned_token_ids).unsqueeze(0).to(device)\n",
    "    }\n",
    "    \n",
    "    flops_analysis = FlopCountAnalysis(model, inputs['input_ids'])\n",
    "    total_flops += flops_analysis.total()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask']\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    generated_summary = tokenizer.decode(\n",
    "        summary_ids[0], \n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    \n",
    "    total_time += (end_time - start_time)\n",
    "    outputs.append(generated_summary)\n",
    "    targets.append(reference_summary)\n",
    "\n",
    "average_flops = total_flops / len(test_dataset)\n",
    "average_time = total_time / len(test_dataset)\n",
    "\n",
    "rouge_results = rouge.compute(predictions=outputs, references=targets)\n",
    "bleu_results = bleu.compute(predictions=outputs, references=targets)\n",
    "\n",
    "print(\"Inference Results:\")\n",
    "print(f\"Average FLOPs: {average_flops:.2e}\")\n",
    "print(f\"Average forward pass time: {average_time:.4f} seconds\")\n",
    "print(\"ROUGE Scores:\", rouge_results)\n",
    "print(\"BLEU Score:\", bleu_results)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
